{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import jax.example_libraries.optimizers as optimizers\n",
    "import pandas as pd\n",
    "import jax\n",
    "from jax import grad, vmap, jit, random\n",
    "# key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalPP(params,lam1,lam2):\n",
    "    mu, k1_1, k2_1, kappa1, alpha1, k1_2, k2_2, kappa2, alpha2 = params\n",
    "\n",
    "    #Structure tensor\n",
    "    a1 = jnp.array([jnp.cos(alpha1),jnp.sin(alpha1),0])\n",
    "    a2 = jnp.array([jnp.cos(alpha2),jnp.sin(alpha2),0])\n",
    "    M1 = jnp.outer(a1,a1)\n",
    "    M2 = jnp.outer(a2,a2)\n",
    "    #Kinematics\n",
    "    lam3 = 1/(lam1*lam2)\n",
    "    F = jnp.array([[(lam1) ,0., 0],[0.,(lam2), 0] ,[0., 0,(lam3)]])\n",
    "    C = F.T*F\n",
    "    invF = jnp.linalg.inv(F)\n",
    "    invC = jnp.linalg.inv(C)\n",
    "    I = jnp.eye(3)\n",
    "    #Invariants\n",
    "    I1 = jnp.trace(C)\n",
    "    I4_1 = jnp.tensordot(C,M1)\n",
    "    I4_2 = jnp.tensordot(C,M2)\n",
    "    #Evaluate stress\n",
    "    H1 = kappa1*I1+(1-3*kappa1)*I4_1\n",
    "    H2 = kappa2*I1+(1-3*kappa2)*I4_2\n",
    "    E1 = H1-1\n",
    "    E2 = H2-1\n",
    "    S2 = mu*I+2*k1_1*jnp.exp(k2_1*E1**2)*E1*(kappa1*I+(1-3*kappa1)*M1)+2*k1_2*jnp.exp(k2_2*E2**2)*E2*(kappa2*I+(1-3*kappa2)*M2)\n",
    "    p = S2[2,2]/invC[2,2] #Get pressure  by equating S_33=0\n",
    "    S = -p*invC+S2\n",
    "    P =  F*S\n",
    "    sigma = jnp.dot(F,jnp.dot(S,F.T))\n",
    "    # return sigma[0,0], sigma[1,1]\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lol=jnp.array(['Control','XR','TE','XRTE'])\n",
    "# Control=jnp.array(['3C'\n",
    "# #                   ,'P12AC1S1'  #R01_12\n",
    "# #                   ,'P12BC2S1'  #R01_12\n",
    "#                   ,'6CS1'\n",
    "#                   ,'P9C1'      #R01_5\n",
    "#                   ,'P10C1S1'   #R01_5\n",
    "#                  ])\n",
    "# XR=jnp.array(['3XC'\n",
    "#              ,'5XCL'\n",
    "#              ,'5XCR'\n",
    "#              ,'6XCS1'\n",
    "#             ])\n",
    "# TE=jnp.array(['3AAA1'\n",
    "#              ,'P10E2A1'   #R01_5\n",
    "#              ,'P9E2A1'    #R01_5\n",
    "#              ,'3TPA1'\n",
    "#             ])\n",
    "# XRTE=jnp.array(['3XAAA1'\n",
    "#                ,'3XTPA1'\n",
    "#                ,'5XTPA1'\n",
    "#                ,'6XAAA1'\n",
    "#                ,'6XTPA1'\n",
    "#                , 'P9E1A1'  #R01_5\n",
    "#               ])\n",
    "# big=jnp.array((Control,XR,TE,XRTE))\n",
    "# print(big[1][0])\n",
    "\n",
    "# READ AND STACK FOR CONTROL\n",
    "\n",
    "OffX_3C=pd.read_csv('Control/PK_lam_OffX_3C.csv').to_numpy()[:,1:]\n",
    "OffX_6CS1=pd.read_csv('Control/PK_lam_OffX_6CS1.csv').to_numpy()[:,1:]\n",
    "OffX_P9C1=pd.read_csv('Control/PK_lam_OffX_P9C1.csv').to_numpy()[:,1:]\n",
    "OffX_P10C1S1=pd.read_csv('Control/PK_lam_OffX_P10C1S1.csv').to_numpy()[:,1:]\n",
    "OffXall=jnp.vstack([OffX_3C,OffX_6CS1,OffX_P9C1\n",
    "#                    ,OffX_P10C1S1\n",
    "                  ])\n",
    "# OffX=jnp.array([OffX_3C,OffX_6CS1,OffX_P9C1\n",
    "#                ,OffX_P10C1S1\n",
    "            #   ])\n",
    "# print(OffX[0])\n",
    "\n",
    "OffY_3C=pd.read_csv('Control/PK_lam_OffY_3C.csv').to_numpy()[:,1:]\n",
    "OffY_6CS1=pd.read_csv('Control/PK_lam_OffY_6CS1.csv').to_numpy()[:,1:]\n",
    "OffY_P9C1=pd.read_csv('Control/PK_lam_OffY_P9C1.csv').to_numpy()[:,1:]\n",
    "# OffY_P10C1S1=pd.read_csv('Control/PK_lam_OffY_P10C1S1.csv').to_numpy()[:,1:]\n",
    "OffYall=jnp.vstack([OffY_3C,OffY_6CS1,OffY_P9C1\n",
    "#                    ,OffY_P10C1S1\n",
    "                  ])\n",
    "# OffY=jnp.array([OffY_3C,OffY_6CS1,OffY_P9C1])\n",
    "# print(OffY[0])\n",
    "\n",
    "Equi_3C=pd.read_csv('Control/PK_lam_Equi_3C.csv').to_numpy()[:,1:]\n",
    "Equi_6CS1=pd.read_csv('Control/PK_lam_Equi_6CS1.csv').to_numpy()[:,1:]\n",
    "Equi_P9C1=pd.read_csv('Control/PK_lam_Equi_P9C1.csv').to_numpy()[:,1:]\n",
    "Equi_P10C1S1=pd.read_csv('Control/PK_lam_Equi_P10C1S1.csv').to_numpy()[:,1:]\n",
    "Equiall=jnp.vstack([Equi_3C,Equi_6CS1,Equi_P9C1\n",
    "#                    ,Equi_P10C1S1\n",
    "                  ])\n",
    "# Equi=jnp.array([Equi_3C,Equi_6CS1,Equi_P9C1,Equi_P10C1S1])\n",
    "# print(Equiall[0,:])\n",
    "\n",
    "allall=jnp.vstack([OffXall,OffYall,Equiall])\n",
    "\n",
    "veclens=[0,len(OffX_3C[:,0]),len(OffX_3C[:,0])+len(OffX_6CS1[:,0]),len(OffX_3C[:,0])+len(OffX_6CS1[:,0])+len(OffX_P9C1[:,0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 74, 149, 209]\n"
     ]
    }
   ],
   "source": [
    "print(veclens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209,)\n",
      "1.0088654\n"
     ]
    }
   ],
   "source": [
    "# all experimental data:\n",
    "lam1 = OffXall[:,0]\n",
    "lam2 = OffXall[:,2]\n",
    "PE1 = OffXall[:,1]\n",
    "PE2 = OffXall[:,3]\n",
    "\n",
    "lam=jnp.vstack([lam1,lam2]).T\n",
    "PE=jnp.vstack([PE1,PE2]).T\n",
    "\n",
    "print(jnp.shape(lam[:,0]))\n",
    "print(lam[:,1][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def loss(params, lam, PE):\n",
    "    # scipy will want x to be a vector rather than an array of arrays or vector of vectors\n",
    "    # let's say that parameters are stacked in 1D\n",
    "    n_samples = 3\n",
    "    n_p_i = 9\n",
    "\n",
    "    # you have somewhere the lamda data as well as the sigma data, read it before hand and store as global variable\n",
    "    # or gets passed to the function as an argument, reading is slow dont have it inside this function\n",
    "    # I know the lamdas and the sigma BOTH change per sample, because of DIC, that's fine, still read ahead of time\n",
    "    # the GLOBAL params\n",
    "    param_global = params[n_samples*n_p_i:(n_samples+1)*n_p_i]\n",
    "    #print(param_global)\n",
    "    # there is a hyper-parameter to determine how correlated these parameters are\n",
    "    # the alpha can be part of the optimization or fixed to a value\n",
    "    alpha=params[-1]\n",
    "    # initialize objective function\n",
    "    obj=0\n",
    "    for i in range(n_samples):\n",
    "        params_i = params[i*n_p_i:(i+1)*n_p_i]\n",
    "        a=veclens[i] #lower bound of each specimen data point\n",
    "        b=veclens[i+1] # upper bound of each specimen data point\n",
    "        # print(b-a)\n",
    "        for n in range(a,b):\n",
    "            P = evalPP(params_i,lam[:,0][n],lam[:,1][n])\n",
    "            obj += (1/(b-a))*((P[0,0]-PE[:,0][n])**2 + (P[1,1]-PE[:,1][n])**2)\n",
    "        for j in range(0,len(params_i)):\n",
    "            obj += (params_i[j]-param_global[j])**2/alpha**2\n",
    "    obj+= alpha**2\n",
    "    return obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000645528\n"
     ]
    }
   ],
   "source": [
    "x = [0.01,1,18,0.2,jnp.pi/4,2,2.5,1,jnp.pi/4\n",
    "   ,0.01,1,18,0.2,jnp.pi/4,2,2.5,1,jnp.pi/4\n",
    "   ,0.01,1,18,0.2,jnp.pi/4,2,2.5,1,jnp.pi/4\n",
    "   ,0.01,1,18,0.2,jnp.pi/4,2,2.5,1,jnp.pi/4\n",
    "   ,0.001]\n",
    "\n",
    "print(loss(x,lam,PE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the gradient of the loss function using JAX's grad function\n",
    "grad_loss = jit(grad(loss, allow_int=True))\n",
    "\n",
    "# Define the function to train the model\n",
    "def train(params_init, lam, PE, num_epochs=2000, learning_rate=0.001):\n",
    "    # Initialize the optimizer\n",
    "    opt_init, opt_update, get_params = optimizers.adam(learning_rate)\n",
    "    opt_state = opt_init(params_init)\n",
    "\n",
    "    # Define the training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Compute the gradient and loss function\n",
    "        g = grad_loss(params_init, lam, PE)\n",
    "        loss_val = loss(params_init, lam, PE)\n",
    "\n",
    "        # Update the parameters\n",
    "        opt_state = opt_update(epoch, g, opt_state)\n",
    "        params_init = get_params(opt_state)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch {}: Loss = {}\".format(epoch, loss_val))\n",
    "\n",
    "    return params_init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 0.0007445280207321048\n",
      "Epoch 100: Loss = 0.0011792660225182772\n",
      "Epoch 200: Loss = 0.0011430946178734303\n",
      "Epoch 300: Loss = 0.0011016405187547207\n",
      "Epoch 400: Loss = 0.001055672182701528\n",
      "Epoch 500: Loss = 0.0010080323554575443\n",
      "Epoch 600: Loss = 0.0009603245416656137\n",
      "Epoch 700: Loss = 0.000913971452973783\n",
      "Epoch 800: Loss = 0.0008706373628228903\n",
      "Epoch 900: Loss = 0.0008306950912810862\n",
      "Epoch 1000: Loss = 0.0007948949933052063\n",
      "Epoch 1100: Loss = 0.0007634639041498303\n",
      "Epoch 1200: Loss = 0.0009275666088797152\n",
      "Epoch 1300: Loss = 0.0008574693347327411\n",
      "Epoch 1400: Loss = 0.0008043167763389647\n",
      "Epoch 1500: Loss = 0.0008671581745147705\n",
      "Epoch 1600: Loss = 0.0008087582536973059\n",
      "Epoch 1700: Loss = 0.0009363843128085136\n",
      "Epoch 1800: Loss = 0.0008449570159427822\n",
      "Epoch 1900: Loss = 0.0010268747573718429\n"
     ]
    }
   ],
   "source": [
    "# Call the train function to optimize the parameters\n",
    "params_init = [0.01,1,18,0.2,jnp.pi/4,2,2.5,1,jnp.pi/4\n",
    "   ,0.01,1,18,0.2,jnp.pi/4,2,2.5,1,jnp.pi/4\n",
    "   ,0.01,1,18,0.2,jnp.pi/4,2,2.5,1,jnp.pi/4\n",
    "   ,0.01,1,18,0.2,jnp.pi/4,2,2.5,1,jnp.pi/4\n",
    "   ,0.01]\n",
    "params_init = jnp.array(params_init)\n",
    "lam = jnp.array([lam1, lam2])\n",
    "PE = jnp.array([PE1, PE2])\n",
    "params_opt = train(params_init, lam, PE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "0.99950033\n",
      "0.99950033\n",
      "0.99950033\n",
      "global\n",
      "0.99950033\n"
     ]
    }
   ],
   "source": [
    "z=1\n",
    "print(len(params_opt[0:9]))\n",
    "print(params_opt[0:9][z])\n",
    "print(params_opt[9:18][z])\n",
    "print(params_opt[18:27][z])\n",
    "print('global')\n",
    "print(params_opt[27:36][z])\n",
    "# print(params_opt[37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.Array'>\n"
     ]
    }
   ],
   "source": [
    "print(type(PE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
